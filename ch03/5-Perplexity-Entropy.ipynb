{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity's Relation to Entropy\n",
    "\n",
    "We introduced perplexity as a way to evaluate n-gram models on a test set. \n",
    "- A better n-gram model is one that assigns a higher probability to the test data\n",
    "- Perplexity is a normalized version of the probability of the test set. \n",
    "\n",
    "The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity.\n",
    "\n",
    "### Entropy\n",
    "- Is a measure of information\n",
    "\n",
    "Given a random variable $X$ ranging over whatever we are predicting (words, letters, parts of speech, the set of which we'll call $\\lambda$) and with a particular probability function, call it $p(x)$, the entropy of the random variable $X$ is:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{x\\in\\lambda}p(x)\\log_2p(x)\n",
    "$$\n",
    "\n",
    "- We can think of entropy as the lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal coding scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of what we will use entropy for involves *sequences*. For a grammar, for example, we will be computing the entropy of some sequence of words $W = \\{w_0, w_1, w_2, \\ldots, w_n\\}$. One way to do this is to have a variable that ranges over sequence of words. For example we can compute the entropy of a random variable that ranges over all finite sequences of words of length $n$ in some language $L$ as follows:\n",
    "\n",
    "$$\n",
    "H(w_1, w_2,\\ldots,w_n) = -\\sum_{w_1^n\\in L} p(W_1^n)\\log p(W_1^n)\n",
    "$$\n",
    "\n",
    "We could define the **entropy rate** (we could also think of this as the **per-word entropy**) as the entropy of this sequence divided by the number of words:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}H(W_1^n) = -\\frac{1}{n}\\sum_{W_1^n\\in L} p(W_1^n)\\log p(W_1^n)\n",
    "$$\n",
    "\n",
    "But to measure the true entropy of a language, we need to consider sequences of infinite length. If we think of a language as a stochastic process $L$ that produces a sequence of words, and allow $W$ to represent the sequence of words $w_1,\\ldots,w_n$, then $L$'s entropy rate $H(L)$ is defined as \n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "H(L) & = \\lim_{n\\rightarrow \\infty}\\frac{1}{n} H(w_1,w_2,\\ldots,w_n)\\\\\n",
    "& = -\\lim_{n\\rightarrow\\infty}\\frac{1}{n}\\sum_{W\\in L} p(w_1,\\ldots,w_n)\\log p(w_1,\\ldots,w_n)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The Shannon-McMillan-Breiman theorem states that if the language is regular in certain ways (to be exact, if it is both stationary and ergodic),\n",
    "\n",
    "$$\n",
    "H(L) = \\lim_{n\\rightarrow\\infty}-\\frac{1}{n}\\log p(w_1,w_2,\\ldots w_n)\n",
    "$$\n",
    "\n",
    "That is, we can take a single sequence that is long enough instead of summing over all possible sequences. The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities. \n",
    "\n",
    "Natural language is not stationary, so statistical models only give an approximation to the correct distributions and entropies of natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy\n",
    "This is useful when we don't know the actual probability distribution $p$ that generated some data. It allows us to use some $m$, which is a model of $p$ (i.e. an approximation to $p$). The cross-entropy of $m$ on $p$ is defined by\n",
    "\n",
    "$$\n",
    "H(p, m) = \\lim_{n\\rightarrow\\infty}-\\frac{1}{n}\\sum_{W\\in L} p(w_1,\\ldots,w_n)\\log m(w_1,\\ldots, w_n)\n",
    "$$\n",
    "\n",
    "That is, we draw sequences according to the probability distribution $p$, but sum the log of their probabilities according to $m$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process:\n",
    "\n",
    "$$\n",
    "H(p, m) = \\lim_{n\\rightarrow\\infty}-\\frac{1}{n}\\log m(w_1w_2\\ldots w_n)\n",
    "$$\n",
    "\n",
    "This means that, as for entropy, we can estimate the cross-entropy of a model $m$ on some distribution $p$ by taking a single sequence that is long enough instead of summing over all possible sequences.\n",
    "\n",
    "What makes the cross-entropy useful is that the cross-entropy $H(p, m)$ is an upper bound on the entropy $H(p)$. For any model $m$:\n",
    "\n",
    "$$\n",
    "H(p) \\le H(p, m)\n",
    "$$\n",
    "\n",
    "This means that we can use some simplified model $m$ to help estimate the true entropy of a sequence of symbols drawn according to probability $p$. The more accurate $m$ is, the closer the cross-entropy $H(p, m)$ will be to the true entropy $H(p)$. Thus, the difference between $H(p, m)$ and $H(p)$ is a measure of how accurate a model is. Between two models $m_1$ and $m_2$, the more accurate model will be the one with the lower cross-entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to see the relation between perplexity and cross-entropy. Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity. We will need an approximation to cross-entropy, relying on a (sufficiently long) sequence of fixed length. This approximation to the cross-entropy of a model $M = P(w_i |w_{i-N+1}\\ldots w_{i-1})$ on a sequence of words $W$ is:\n",
    "\n",
    "$$\n",
    "H(W) = -\\frac{1}{N}\\log P(w_1w_2\\ldots w_N)\n",
    "$$\n",
    "\n",
    "The **perplexity** of a model $P$ on a sequence of words $W$ is now formally defined as the exponential of this cross-entropy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{Perplexity}(W) = & 2^{H(W)} \\\\\n",
    " = & P(w_1w_2\\ldots w_N)^{-\\frac{1}{N}} \\\\\n",
    " = & \\sqrt[N]{\\frac{1}{P(w_1w_2\\ldots w_N)}}\\\\\n",
    " = & \\sqrt[N]{\\prod_{i=1}^N\\frac{1}{P(w_1 |w_1\\ldots w_{i-1})}}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "The last line is by the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.\n",
    "- n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the **maximum likelihood estimate**)\n",
    "- n-gram **language models** are evaluated extrinsically in some task, or intrinsically using **perplexity**.\n",
    "- The **perplexity** of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model\n",
    "- **Smoothing** algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through **backoff** or **interpolation**.\n",
    "- Both backoff and interpolation require **discounting** to create a probability distribution.\n",
    "- **Kneser-Ney** smoothing makes use of the probability of a word being a novel **continuation**. The interpolated **Kneser-Ney** smoothing algorithm mixes a discounted probability with a lower order continuation probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
