{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Maximum Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_counts import count_ngrams\n",
    "from base_ngram_model import BaseNgramModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "def create_vocab_sents():\n",
    "    df = pd.read_csv('../data/corpus.csv', header=None)\n",
    "    df = df[df[0].str.len() > 20]\n",
    "    \n",
    "    text = df[0][:1000].values\n",
    "    sents = [t.split() for t in text]\n",
    "    tokens = [j for i in sents for j in i]\n",
    "    vocab = Counter(tokens)\n",
    "    return vocab, sents\n",
    "\n",
    "vocab, sents = create_vocab_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.93036113569795\n"
     ]
    }
   ],
   "source": [
    "trigram_model = BaseNgramModel(count_ngrams(3, vocab, sents))\n",
    "print(trigram_model.perplexity(sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3934211547457294\n"
     ]
    }
   ],
   "source": [
    "fivegram_model = BaseNgramModel(count_ngrams(5, vocab, sents))\n",
    "print(fivegram_model.perplexity(sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "To deal with the problem that words in our vocabulary may appear in a test set in an unseen context (e.g. appear after a word they never appeared after in training). \n",
    "\n",
    "To prevent language model from assigning zero probability to these unseen events, we'll have to shave off a bit of probability mass from some more frequent events and give it to the events we've never seen. This process is called **smoothing** or **discounting**. \n",
    "\n",
    "### Laplace Smoothing\n",
    "The simplest way to do smoothing is to add one to all the bigram counts, before we normalize them into probabilities. All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. \n",
    "\n",
    "Laplace smoothing does not perform well enough to be used in model n-gram models, but we use it to illustrate the concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-k smoothing\n",
    "One algernative to add-one smoothing is to move a bit less of the probability mass from the seen to the unseen events. Instead of adding 1 to each count, we add a fractional count *k* (0.5? 0.05? 0.01?). This algorithm is therefore called **add-k smoothing**.\n",
    "\n",
    "$$\n",
    "P^*_{\\mathrm{Add-k}}(w_n|w_{n-1}) = \\frac{C(w_{n-1}w_n) + k}{C(w_{n-1})+kV},\n",
    "$$\n",
    "where $V$ is the vocabulary and $C(*)$ is the count.\n",
    "\n",
    "Add-k smoothing requires that we have a method for choosing *k*; this can be done, for example, by optimizing on a **devset**. Although add-k is useful for some tasks (including text classification), it turns out that it still doesn't work well for language modelling, generating counts with poor variances and often inappropriates discounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smoothing import AddKNgramModel, LaplaceNgramModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.28545491314981\n"
     ]
    }
   ],
   "source": [
    "addkgram_model = AddKNgramModel(0.01, count_ngrams(3, vocab, sents))\n",
    "print(addkgram_model.perplexity(sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164.6917245427675\n"
     ]
    }
   ],
   "source": [
    "addkgram_model = AddKNgramModel(0.05, count_ngrams(3, vocab, sents))\n",
    "print(addkgram_model.perplexity(sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689.6801055325047\n"
     ]
    }
   ],
   "source": [
    "laplace_model = LaplaceNgramModel(count_ngrams(3, vocab, sents))\n",
    "print(laplace_model.perplexity(sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backoff and Interpolation\n",
    "So far we are trying to resolve the problem of zero frequency n-grams. But there is an additional source of knowledge we can draw on. If we are trying to compute $P(w_n|w_{n-2}w_{n-1})$ but we have no examples of a particular trigram $w_{n-2}w_{n-1}w_n$. We can instead estimate its probability by using the bigram probability $P(w_n|w_{n-1})$. Similarly, if we don't have counts to compute $P(w_n|w_{n-1})$, we can look to the unigram $P(w_n)$.\n",
    "\n",
    "In other words, sometimes using **less context** is a good thing, helping to generalize more for contexts that the model hasn't learned much about. There are two ways to use this n-gram \"hierarchy\". \n",
    "\n",
    "### Backoff\n",
    "We use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram. I.e. we only 'back off' to a lower-order n-gram if we have zero evidence for a higher-order n-gram.\n",
    "\n",
    "### Interpolation\n",
    "We always mix the probability estimates from all the n-gram estimators, weighting and combining the trigram, bigram, and unigram counts.\n",
    "\n",
    "Simple linear interpolation: we combine different order n-grams by linearly interpolating all the model. Thus, we estimate the trigram probability $P(w_n|w_{n-2}w_{n-1})$ by mixing together the unigram, bigram and trigram probabilities, each weighted by a $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "\\hat{P}(w_n|w_{n-2}w_{n-1})  & = \\lambda_1 P(w_n|w_{n-2}w_{n-1})\\\\\n",
    " & + \\lambda_2 P(w_n|w_{n-1})\\\\\n",
    " & + \\lambda_3 P(w_n)\n",
    " \\end{array}\n",
    " $$\n",
    " such that $\\lambda$s sum to 1:\n",
    " \n",
    " $$\n",
    " \\sum_i\\lambda_i = 1\n",
    " $$\n",
    " \n",
    "In a slightly more sophisticated version of linear interpolation, each $\\lambda$ weight is computed by conditioning on the context. This way, if we have particularly accurate accounts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the $\\lambda$s for those trigrams higher and thus give that trigram more weight in the interpolation. \n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "\\hat{P}(w_n|w_{n-2}w_{n-1}) & = \\lambda_1(w_{n-2}^{n-1}) P(w_n|w_{n-2}w_{n-1})\\\\\n",
    " & + \\lambda_2(w_{n-2}^{n-1})P(w_n|w_{n-1})\\\\\n",
    " & + \\lambda_3(w_{n-2}^{n-1})P(w_n)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "How are these $\\lambda$ values set? Both the simple interpolation and conditional interpolation $\\lambda$s are learned from a **held-out** corpus. A held-out corpus is an additional training corpus that we use to set hyperparameters like these $\\lambda$ values, by choosing the $\\lambda$ values that maximize the likelihood of the held-out corpus. \n",
    "\n",
    "That is, we fix the n-gram probabilities and then search for the $\\lambda$ values that when plugged into the linear equations give us the highest probability of the held-out set. There are various ways to find the optimal set of $\\lambda$s. One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimzal $\\lambda$s.\n",
    "\n",
    "In a **backoff** n-gram model, if the n-gram we need has zero counts, we approximate it by backing off to the (N-1)-gram. We continue backing off until we reach a history that has some counts.\n",
    "\n",
    "In order for a backoff model to give a correct probability distribution, we have to **discount** the higher-order n-grams to save some probability mass for the lower order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren't discounted and we just used the undiscounted MLE probability, then as soon as we replaced an n-gram which has zero probability with a lower-order n-gram, we could be adding probability mass, and the total probability assigned to all possible strings by the language model would be greater than 1! In addition to this explicit discount factor, we'll need a function $\\alpha$ to distribute this probability mass to the lower order n-grams.\n",
    "\n",
    "This kind of backoff with discounting is also called **Katz backoff**. In Katz backoff we rely on a discounted probability *P\\** if we've seen this n-gram before (i.e. if we have non-zero counts). Otherwise, we cursively back off to the Katz probability for the shorter-history (N-1)-gram. The probability for a backoff n-gram $P_{\\mathrm{BO}}$ is thus computed as follows:\n",
    "\n",
    "$$\n",
    "P_{\\mathrm{BO}}(w_n|w_{n-N+1}^{n-1}) = \\begin{cases}P^*(w_n|w_{n-N+1}^{n-1}), & \\mathrm{if}\\,C(w_{n-N+1}^n > 0\\\\\n",
    "\\alpha(w_{n-N+1}^{n-1})P_{\\mathrm{BO}}(w_n|w_{n-N+2}^{n-1}), & \\mathrm{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Katz backoff is often combined with a smoothing method called **Good-Turing**. This involves quite detailed computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney Smoothing\n",
    "Kneser-Ney has its root in a method called **absolute discounting**. \n",
    "\n",
    "### Absolute Discounting\n",
    "Recall that **discounting** of the counts for frequent n-grams is necessary to save some probability mass for the smoothing algorithm to distribute to the unseen n-grams.\n",
    "\n",
    "Consider an n-gram that has count 4. We need to discount this count by some amount. But how much should we discount it? Church and Gale (1991)'s clever idea was to look at a held-out corpus and just see what the count is for all those bigrams that had count 4 in the training set. The computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words. On average, a bigram that occurred 4 times in the first 22 million words occurred 3.23 times in the next 22 million words. \n",
    "\n",
    "|Bigram count in training set | Bigram count in heldout set |\n",
    "|---------|----------| \n",
    "| 0 | 0.0000270 |\n",
    "| 1 | 0.448 |\n",
    "| 2 | 1.25 |\n",
    "| 3 | 2.24 |\n",
    "| 4 | 3.23 |\n",
    "| 5 | 4.21 |\n",
    "| 6 | 5.23 |\n",
    "| 7 | 6.21 |\n",
    "| 8 | 7.21 |\n",
    "| 9 | 8.26 |\n",
    "\n",
    "The astute reader may have noticed that except for the held-out counts for 0 and 1, all the other bigram counts in the held-out set could be estimated pretty well by just subtracting 0.75 from the count in the training set! **Absolute discounting** formalizes this intuition by subtracting a fixed (absolute) discount *d* from each count. The intuition is that since we have good estimates already for the very high counts, a small amount *d* won't affect them much. It will mostly affect small amounts, for which we don't necessarily trust the estimate anyway, and the discount is actually a good one for bigrams with counts 2 through 9. The equation for interpolated absolute discounting applied to bigrams:\n",
    "\n",
    "$$\n",
    "P_{\\mathrm{AbsoluteDiscounting}}(w_i|w_{i-1}) = \\frac{C(w_{i-1}w_i)-d}{\\sum_v C(w_{i-1}v)} + \\lambda(w_{i-1})P(w_i)\n",
    "$$\n",
    "\n",
    "The first term is the discounted bigram, and the second term is the unigram with an interpolation weight $\\lambda$. We could just set all the *d* values to 0.75, or we could keep a separate discount value of 0.5 for the bigrams with counts of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kneser-Ney discounting \n",
    "Augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution. Instead of $P(w)$, which answers the question \"How likely is $w$?\", we'd like to create a unigram model that we might call $P_{\\mathrm{CONTINUATION}}$, which answers the question \"How likely is $w$ appear as a novel continuation?\". \n",
    "\n",
    "The Kneser-Ney intuition is to base our estimate of $P_{\\mathrm{CONTINUATION}}$ on the *number of different contexts word w has appeared in*, that is, the number of bigram types it completes. Every bigram type was a novel continuation the first time it was seen. We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well. The number of times a word $w$ appears as a novel continuation can be expressed as:\n",
    "\n",
    "$$\n",
    "P_{\\mathrm{CONTINUATION}}(w)\\propto |\\{v: C(vw) > 0\\}|\n",
    "$$\n",
    "\n",
    "To turn this count into a probability, we normalize by the total number of word bigram types. In summary:\n",
    "\n",
    "$$\n",
    "P_{\\mathrm{CONTINUATION}}(w) = \\frac{\\{v: C(vw) > 0 \\}|}{|\\{(u', w') : C(u'w') > 0\\}|}\n",
    "$$\n",
    "\n",
    "An equivalent formulation based on a different metaphor is to use the number of word types seen to precede $w$, normalized by the number of words preceding all words, as follows:\n",
    "\n",
    "$$\n",
    "P_{\\mathrm{CONTINUATION}}(w) = \\frac{|\\{v: C(vw) > 0 \\}|}{\\sum_{w'}|\\{v: C(vw') > 0 \\}|}\n",
    "$$\n",
    "\n",
    "A frequent word (Kong) occurring in only one context (Hong) will have a low continuation probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final equation for **Intepolated Kneser-Ney** smoothing for bigrams is then:\n",
    "\n",
    "$$\n",
    "P_{\\mathrm{KN}}(w_i|w_{i-1}) = \\frac{\\max(C(w_{i-1}w_i)-d, 0)}{C(w_{i-1})} + \\lambda(w_{i-1})P_{\\mathrm{CONTINUATION}}(w_i)\n",
    "$$\n",
    "\n",
    "The $\\lambda$ is normalizing constant that is used to distribute the probability mass we've discounted:\n",
    "\n",
    "$$\n",
    "\\lambda(w_{i-1}) = \\frac{d}{\\sum_v C(w_{i-1}v)}|\\{w:C(w_{i-1}w)> 0\\}|\n",
    "$$\n",
    "\n",
    "The first term, $\\frac{d}{\\sum_v C(w_{i-1}v)}$, is the normalized discount. The second term, $|\\{w: C(w_{i-1}w) > 0\\}|$, is the number of word types that can follow $w_{i-1}$ or equivalently, the number of word types that we discounted; in other words, the number of times we applied the normalized discount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smoothing import KneserNeyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = count_ngrams(3, vocab, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KneserNeyModel(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(input_text):\n",
    "    tokenized = nltk.word_tokenize(input_text)\n",
    "    if len(tokenized) < 2:\n",
    "        response = \"Say more.\"\n",
    "    else:\n",
    "        completions = {}\n",
    "        for sample in model.samples():\n",
    "            if (sample[0], sample[1]) == (tokenized[-2], tokenized[-1]):\n",
    "                completions[sample[2]] = model.prob(sample)\n",
    "        if len(completions) == 0:\n",
    "            response = \"Can we talk about something else?\"\n",
    "        else:\n",
    "            best = max(\n",
    "                completions.keys(), key=(lambda key: completions[key])\n",
    "            )\n",
    "            tokenized += [best]\n",
    "            response = \" \".join(tokenized)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove credit report agency\n"
     ]
    }
   ],
   "source": [
    "print(complete('remove credit report'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
