{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Language Models\n",
    "\n",
    "### Perplexity\n",
    "In practice we don't use raw probability as our metric for evaluating language models, but a variant called **perplexity**. The **perplexity** of a language model on a test set is the inverse probability of the test set, normalized by the number of words.\n",
    "\n",
    "For a test set $W=w_1w_2\\ldots w_N$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{PP}(W) & = P(w_1w_2\\ldots w_N)^{-\\frac{1}{N}} \\\\\n",
    "               & = \\sqrt[N]{\\frac{1}{P(w_1w_2\\ldots w_N)}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can use the chain rule to expand the probability of $W$:\n",
    "\n",
    "$$\n",
    "\\mathrm{PP}(W) = \\sqrt[N]{\\prod_{i=1}^N\\frac{1}{P(w_i|w_1\\ldots w_{i-1})}}\n",
    "$$\n",
    "\n",
    "Thus, if we are computing the perplexity of $W$ with a bigram language model, we get\n",
    "\n",
    "$$\n",
    "\\mathrm{PP}(W) = \\sqrt[N]{\\prod_{i=1}^N\\frac{1}{P(w_i|w_{i-1})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because of the inverse inside the normalization, the higher the conditional probability of the word sequence, the lower the perplexity. Thus, minimizing perplexity is equivalent to maximizing the test set probability according to the lanuguage model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
