{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector semantics\n",
    "The idea of vector semantics is to represent a word as a point in some multidimensional semantic space. Vectors for represending words are generally called **embeddings**, because the word is embedded in a particular vector space. \n",
    "\n",
    "Vector or distributional models of meaning are generally based on a **co-occurrence matrix**, a way of representing how often words co-occur. This matrix can be constructed in various ways; let's begin by looking at one such occurrence matrix, a term-document matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors and documents\n",
    "In a **term-document matrix**, each row represents a word in  the vocabulary, and each column represents a document from some collection of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Mutual Information (PMI)\n",
    "An alternative weighting function to tf-idf is called PPMI (positive pointwise mutual information). PPMI draws on the intuition that the best way to weight the association between words is to ask how much **more** the two words co-occur in our corpus than we would have a priori expected them to appear by chance.\n",
    "\n",
    "Pointwise mutual information is one of the most important concepts in NLP. It is a measure of how often two events *x* and *y* occur, compared with what we would expect if they were independent:\n",
    "\n",
    "$$\n",
    "I(x, y) = \\log_2\\frac{P(x, y)}{P(x) P(y)}\n",
    "$$\n",
    "\n",
    "The pointwise mutual information between a target word $w$ and a context word $c$ is then defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{PMI}(w, c) = \\log_2\\frac{P(w, c)}{P(w) P(c)}\n",
    "$$\n",
    "\n",
    "The numerator tells us how often we observed the two words together (assuming we compute probability by using the MLE). The denominator tells us how often we would **expect** the two words to co-occur assuming they each occurred independently; recall that the probability of two independent events both occurring is just the product of the probabilities of the two events.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
