{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Normalization\n",
    "Task of putting words/tokens in a standard format, choosing a single normal form for words with multiple forms like USA and US.\n",
    "\n",
    "### Case folding\n",
    "Maps everything to lower case, helpful for generalization in tasks such as information retrieval or speech recognition. However, for sentiment analysis and other text classification tasks, case can be quite helpful and case folding is generally not done.\n",
    "\n",
    "### Lemmatization\n",
    "Determining whether two words have the same root, despite their surface differences.\n",
    "\n",
    "How is it done? The most sophisticated methods for lemmatization involve complete **morphological parsing** of the word. **Morphology** is the study of the way words are built up from smaller meaning-bearing units called **morphemes**. Two broad classes of morphemes can be distinguished:\n",
    "\n",
    "- **stems**: the central morpheme of the word, supplying the main meaning\n",
    "- **affixes**: adding additional meanings of various kinds.\n",
    "\n",
    "One useful lemmatizer is the `WordNetLemmatizer` from `nltk`.\n",
    "\n",
    "This uses [WordNet](https://wordnet.princeton.edu/)'s built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    WordNet Lemmatizer\n",
      "\n",
      "    Lemmatize using WordNet's built-in morphy function.\n",
      "    Returns the input word unchanged if it cannot be found in WordNet.\n",
      "\n",
      "        >>> from nltk.stem import WordNetLemmatizer\n",
      "        >>> wnl = WordNetLemmatizer()\n",
      "        >>> print(wnl.lemmatize('dogs'))\n",
      "        dog\n",
      "        >>> print(wnl.lemmatize('churches'))\n",
      "        church\n",
      "        >>> print(wnl.lemmatize('aardwolves'))\n",
      "        aardwolf\n",
      "        >>> print(wnl.lemmatize('abaci'))\n",
      "        abacus\n",
      "        >>> print(wnl.lemmatize('hardrock'))\n",
      "        hardrock\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Lemmatization work in SpaCy?\n",
    "Spacy is another NLP library which can perform lemmatization (amongst lots of other things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute compute\n",
      "computer computer\n",
      "computed compute\n",
      "computing computing\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp(u'compute computer computed computing')\n",
    "for word in sentence:\n",
    "    print(word.text,  word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of SpaCy's lemmatizer is given in this [Stackoverflow Q/A](https://stackoverflow.com/questions/43795249/how-does-spacy-lemmatizer-works), but basically it is based on `_morphy` from `nltk` and includes some new punctuation rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is a simpler but cruder method of morphological analysis. One of the most widely used stemming algorithms is the Porter Stemmer. Another is the Snowball stemmer. You can call both from `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "          'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need to specify the language to use the `SnowballStemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "          'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are clearly some problems with stemming, sometimes completely changes the meaning of a word, as in `colonizer` -> `colon`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 Sentence Segmentation\n",
    "**Sentence segmentation** is another important step in text processing. The most useful cues for segmenting a text into sentences are punctuations, like periods, question marks and exclamation points. Due to the ambiguity of period characters, sentences tokenization and word tokenization may be addressed jointly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
