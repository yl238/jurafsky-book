{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "At least three tasks are commonly applied as part of any normalization process\n",
    "1. Tokenizing (segmenting) words\n",
    "2. Normalizing word formats\n",
    "3. Segmenting sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "1. Regular expression tokenizers\n",
    "A `RegexTokenizer` splits a string into substrings using a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That',\n",
       " 'U.S.A.',\n",
       " 'poster-print',\n",
       " 'costs',\n",
       " '$12.40',\n",
       " '...',\n",
       " '52%',\n",
       " 'and',\n",
       " 'more',\n",
       " ',',\n",
       " 'and',\n",
       " 'one',\n",
       " ',',\n",
       " 'two',\n",
       " ',',\n",
       " 'three',\n",
       " '!']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "pattern=r'''(?x)\n",
    "\\d+%?\n",
    "|\\w+[-]*\\w+\n",
    "|[a-zA-Z\\.]+\n",
    "|\\$?\\d+\\.\\d+\n",
    "|\\.\\.\\.\n",
    "|[][.,;\"’?!():-_‘]\n",
    "'''\n",
    "text = 'That U.S.A. poster-print costs $12.40...52% and more, and one, two, three!'\n",
    "regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-pair encoding BPE\n",
    "Use a kind of tokenization in which most tokens are words, but some tokens are frequent morphemes or other subwords like *-er*, so an unseen word can be represented by combining the parts.\n",
    "\n",
    "The intuition of the BPE algorithm is to iteratively merge frequent pairs of characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 'r')\n",
      "('er', '</w>')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('new', 'er</w>')\n",
      "('low', '</w>')\n",
      "('w', 'i')\n",
      "('wi', 'd')\n",
      "('wid', 'er</w>')\n",
      "('low', 'e')\n",
      "('lowe', 's')\n",
      "('lowes', 't')\n",
      "('lowest', '</w>')\n",
      "('new', '</w>')\n"
     ]
    }
   ],
   "source": [
    "vocab = {'l o w </w>': 5, 'l o w e s t </w>': 2,\n",
    "        'n e w e r </w>': 6, 'w i d e r </w>': 3, 'n e w </w>': 2}\n",
    "num_merges = 16\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordpiece and Greedy Tokenization\n",
    "Like the BPE algorithm, the **wordpiece** algorithm starts with some simple tokenization (such as by whitespaces) into rough words, then breaks those rough word tokens into subword tokens. The **wordpiece** model differs from BPE only in that the special word-boundary token __ appears at the beginning of words rather than at the end, and in the way it merges pairs.\n",
    "\n",
    "Rather than merging the pairs that are most *frequent*, wordpiece instead merges the pairs that minimizes the language model likelihood of the training data. I.e. it chooses two tokens to combine that would give the training corpus the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing max_match_word_segmenter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile max_match_word_segmenter.py\n",
    "class MaxMatchWordSegmenter:\n",
    "    \"\"\"\n",
    "    Basic max-match implementation for word segmentation using a given dictionary.\n",
    "    Tends to have very bad results for English.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dictionary):\n",
    "        \"\"\"\n",
    "        :param dictionary: dictionary containing all words that may be in given strings\n",
    "        \"\"\"\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def segment_words(self, string):\n",
    "        \"\"\"\n",
    "        Segments a sentence into words using the max-match algorithm.  This will attempt to greedily find the largest\n",
    "        words in a sentence, starting at the beginning and moving left-to-right with the remaining string.\n",
    "        :param string: words without spaces separating them\n",
    "        :return: list of words that are a word segmentation of the given string\n",
    "        \"\"\"\n",
    "        words = []\n",
    "\n",
    "        word_begin = 0\n",
    "        while word_begin < len(string):\n",
    "            word = self.find_longest_word(string[word_begin:])\n",
    "            words.append(word)\n",
    "            word_begin += len(word)\n",
    "\n",
    "        return words\n",
    "\n",
    "    def find_longest_word(self, string):\n",
    "        \"\"\"\n",
    "        Finds the longest word that is a prefix of a given string\n",
    "        :param string: string for which to find the longest word prefix\n",
    "        :return: longest prefix of the given string, or the first letter of the string if there is no word prefix\n",
    "        \"\"\"\n",
    "        word_end = len(string)\n",
    "        while word_end > 1:\n",
    "            test_word = string[:word_end]\n",
    "            if self.dictionary.is_word(test_word):\n",
    "                return test_word\n",
    "            word_end -= 1\n",
    "        return string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
