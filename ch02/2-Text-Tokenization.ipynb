{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "At least three tasks are commonly applied as part of any normalization process\n",
    "1. **Tokenizing** (segmenting) words\n",
    "2. Normalizing word formats\n",
    "3. Segmenting sentences\n",
    "\n",
    "This notebook focuses on tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Regular expression tokenizers\n",
    "A `RegexTokenizer` splits a string into substrings using a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That',\n",
       " 'U.S.A.',\n",
       " 'poster-print',\n",
       " 'costs',\n",
       " '$12.40',\n",
       " '...',\n",
       " '52%',\n",
       " 'and',\n",
       " 'more',\n",
       " ',',\n",
       " 'and',\n",
       " 'one',\n",
       " ',',\n",
       " 'two',\n",
       " ',',\n",
       " 'three',\n",
       " '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "pattern=r'''(?x)     # set flag to allow verbose regexps\n",
    "\\d+%?                # percentages\n",
    "|\\w+[-]*\\w+          # words with optional internal hyphens\n",
    "|[a-zA-Z\\.]+         # abbreviations, e.g. U.S.A.\n",
    "|\\$?\\d+\\.\\d+         # currency\n",
    "|\\.\\.\\.              # ellipsis\n",
    "|[][.,;\"’?!():-_‘]   # these are separate tokens\n",
    "'''\n",
    "text = 'That U.S.A. poster-print costs $12.40...52% and more, and one, two, three!'\n",
    "regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Byte-pair encoding BPE\n",
    "Use a kind of tokenization in which most tokens are words, but some tokens are frequent morphemes or other subwords like *-er*, so an unseen word can be represented by combining the parts.\n",
    "\n",
    "The intuition of the BPE algorithm is to iteratively merge frequent pairs of characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 'r')\n",
      "('er', '</w>')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('new', 'er</w>')\n",
      "('low', '</w>')\n",
      "('w', 'i')\n",
      "('wi', 'd')\n",
      "('wid', 'er</w>')\n",
      "('low', 'e')\n",
      "('lowe', 's')\n",
      "('lowes', 't')\n",
      "('lowest', '</w>')\n",
      "('new', '</w>')\n"
     ]
    }
   ],
   "source": [
    "vocab = {'l o w </w>': 5, 'l o w e s t </w>': 2,\n",
    "        'n e w e r </w>': 6, 'w i d e r </w>': 3, 'n e w </w>': 2}\n",
    "num_merges = 16\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wordpiece and Greedy Tokenization\n",
    "Like the BPE algorithm, the **wordpiece** algorithm starts with some simple tokenization (such as by whitespaces) into rough words, then breaks those rough word tokens into subword tokens. The **wordpiece** model differs from BPE only in that the special word-boundary token __ appears at the beginning of words rather than at the end, and in the way it merges pairs.\n",
    "\n",
    "Rather than merging the pairs that are most *frequent*, wordpiece instead merges the pairs that minimizes the language model likelihood of the training data. I.e. it chooses two tokens to combine that would give the training corpus the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxmatch(string, dictionary, gather):\n",
    "    word_end = len(string)\n",
    "    while word_end > 1:\n",
    "        firstword = string[:word_end]\n",
    "        remainder = string[word_end:]\n",
    "        if firstword in dictionary:\n",
    "            gather.append(firstword)\n",
    "            return maxmatch(remainder, dictionary, gather)\n",
    "        word_end -= 1\n",
    "    return gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = [\"day\", \"in\", \"tent\", \"intent\", \"happy\", \"##tent\", \"##tention\", \"##tion\", \"#ion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'day']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxmatch('happyday', dictionary, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intent']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxmatch('intention', dictionary, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenizer in spaCy\n",
    "SpaCy's tokenizer is a rules-based system. Tokenization is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token. Each Doc consists of individual tokens, and we can iterate over them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the raw text is split on whitespace characters, similar to `text.split(' ')`. Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "1. Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, `“do”` and `“n’t”`, while `“U.K.”` should always remain one token.\n",
    "2. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.\n",
    "\n",
    "If there’s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split **complex, nested tokens** like combinations of abbreviations and multiple punctuation marks.\n",
    "\n",
    "<img src=\"spacy_tokenization.svg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer exception: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied.\n",
    "```\n",
    "Prefix: Character(s) at the beginning, e.g. $, (, “, ¿.\n",
    "Suffix: Character(s) at the end, e.g. km, ), ”, !.\n",
    "Infix: Character(s) in between, e.g. -, --, /, ….\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
